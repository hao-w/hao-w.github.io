<div class="research">
<h2>Research</h2>

<div class='paper row'>
  <div class="paper-image col-lg-3">
    <img alt="wu_cebm_2021" src="/assets/thumbnails/wu_cebm_2021.svg">
  </div>
  <div class="paper-info col-lg-9">
    <div class="paper-title">
    Conjugate Energy-Based Models
    </div>
    <div class='abstract'>
      We propose conjugate energy-based models (CEBMs), a class of deep latent-variable models with a tractable posterior. CEBMs have similar use cases as variational autoencoders, in the sense that they learn an unsupervised mapping between data and latent variables. However these models omit a generator, which allows them to learn more flexible notions of similarity between data points. Our experiments demonstrate that CEBMs achieve competitive results in terms of image modelling, predictive power of latent space, and out-of-distribution detection on a variety of datasets.
    <a href="https://openreview.net/forum?id=4k58RmAD02">[Read More]</a>
  </div>

  </div>
</div>


<div class='paper row'>
  <div class="paper-image col-lg-3">
    <img alt="wu_amortized_2020" src="/assets/thumbnails/wu_amortized_2020.gif">
  </div>
  <div class="paper-info col-lg-9">
    <div class="paper-title">
    Amortized Population Gibbs Samplers with Neural Sufficient Statistics
    </div>
    <div class='abstract'>
      We develop amortized population Gibbs (APG) samplers, a class of scalable methods that frame structured variational inference as adaptive importance sampling. APG samplers construct high-dimensional proposals by iterating over updates to lower-dimensional blocks of variables. We train each conditional proposal by minimizing the inclusive KL divergence with respect to the conditional posterior. To appropriately account for the size of the input data, we develop a new parameterization in terms of neural sufficient statistics. Experiments show that APG samplers can be used to train highly-structured deep generative models in an unsupervised manner, and achieve substantial improvements in inference accuracy relative to standard autoencoding variational methods.

    <a href="http://proceedings.mlr.press/v119/wu20h.html">[Read More]</a>
    </div>
  </div>

</div>
